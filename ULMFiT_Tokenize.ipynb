{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ULMFiT + Siamese Network for Sentence Vectors\n",
    "## Part One: Tokenizing\n",
    "This notebook will tokenize the sentences from the SNLI dataset for use in the next notebook\n",
    "\n",
    "### You must have the fastai library installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "%matplotlib inline\n",
    "from fastai.text import *\n",
    "import json\n",
    "import html\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from functools import partial\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils \n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import dataset, dataloader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "import data\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from gensim.models import LdaModel\n",
    "from gensim import models, corpora, similarities\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import time\n",
    "from nltk import FreqDist\n",
    "from scipy.stats import entropy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas.io.json import json_normalize #package for flattening json in pandas df\n",
    "from IPython.display import display\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import logging\n",
    "from ipyexperiments import IPyExperimentsPytorch\n",
    "from ipygpulogger import IPyGPULogger\n",
    "import itertools\n",
    "import joblib\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "TRAINDATAPATH = \"data/PAN14/pan14_train_english-essays/\"\n",
    "TESTDATAPATH = \"data/PAN14/pan14_test01_english-essays/\"\n",
    "FNAMES = ['known01','known02','known03','known04','known05', 'unknown']\n",
    "KNOWN=['known01','known02','known03','known04','known05']\n",
    "\n",
    "token_files = './data/tokens/'\n",
    "snli_root = './data/snli_1.0/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  permute(row):\n",
    "    docs=[row[col] for col in KNOWN if row[col] is not None]\n",
    "    return list(itertools.combinations(docs, 2))\n",
    "\n",
    "def match_unknowns(path):\n",
    "    ds=pd.read_json(path+'/truth.json')\n",
    "    ds=json_normalize(ds['problems'])\n",
    "    ds['known01']=None\n",
    "    ds['known02']=None\n",
    "    ds['known03']=None\n",
    "    ds['known04']=None\n",
    "    ds['known05']=None\n",
    "    ds['unknown']=None\n",
    "    ds.set_index('name', drop=True, inplace=True)\n",
    "    ds=ds[['known01','known02','known03','known04','known05', 'unknown', 'answer']]\n",
    "    dirs = []\n",
    "    docs = []\n",
    "\n",
    "    for i, x in enumerate(os.walk(path)):\n",
    "        if i:\n",
    "            for fname in x[2]:\n",
    "                with open(path+dirs[i-1]+'/'+fname, 'r') as f:\n",
    "                    text = f.read()\n",
    "                    doc = text.strip()\n",
    "                    ds.loc[dirs[i-1],fname[:-4]]=doc\n",
    "        else:\n",
    "            dirs = x[1]\n",
    "            \n",
    "    grouped=ds.groupby(['unknown'])\n",
    "    dupes=[]\n",
    "    for utext, group in grouped:\n",
    "        if len(group.index) > 1:\n",
    "            dupes.append(group)\n",
    "\n",
    "\n",
    "    newrows=pd.DataFrame(columns=['known01','known02','known03','known04','known05', 'unknown'])\n",
    "    for dupe in dupes:\n",
    "        dupe.reset_index(drop=True, inplace=True)\n",
    "        yes=dupe.loc[dupe.answer == \"Y\"]\n",
    "        yes.reset_index(drop=True, inplace=True)\n",
    "        no=dupe.loc[dupe.answer == \"N\"]\n",
    "        no.reset_index(drop=True, inplace=True)\n",
    "        for col in ['known01','known02','known03','known04','known05']:\n",
    "            if no[col] is not None:\n",
    "                newrows=newrows.append(pd.DataFrame(data={'known01':yes.known01,'known02':yes.known02,\n",
    "                                                          'known03':yes.known03, 'known04':yes.known04,\n",
    "                                                          'known05':yes.known05,'unknown':no[col], \n",
    "                                                          'answer':'N'}), sort=False)\n",
    "    newrows=newrows.dropna(subset=['unknown'])\n",
    "\n",
    "    #docs=[d for d in docs if d is not None]\n",
    "    return newrows\n",
    "\n",
    "def read_dataset(path):\n",
    "    ds=pd.read_json(path+'/truth.json')\n",
    "    ds=json_normalize(ds['problems'])\n",
    "    ds['known01']=None\n",
    "    ds['known02']=None\n",
    "    ds['known03']=None\n",
    "    ds['known04']=None\n",
    "    ds['known05']=None\n",
    "    ds['unknown']=None\n",
    "    ds.set_index('name', drop=True, inplace=True)\n",
    "    ds=ds[['known01','known02','known03','known04','known05', 'unknown', 'answer']]\n",
    "    dirs = []\n",
    "    docs = []\n",
    "\n",
    "    for i, x in enumerate(os.walk(path)):\n",
    "        if i:\n",
    "            for fname in x[2]:\n",
    "                with open(path+dirs[i-1]+'/'+fname, 'r') as f:\n",
    "                    text = f.read()\n",
    "                    doc = text.strip()\n",
    "                    docs.append(doc)\n",
    "                    ds.loc[dirs[i-1],fname[:-4]]=doc\n",
    "        else:\n",
    "            dirs = x[1]\n",
    "\n",
    "    return ds, docs\n",
    "\n",
    "\n",
    "train, docs = read_dataset(TRAINDATAPATH)\n",
    "test, _ = read_dataset(TESTDATAPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['yes_pairs']=train.apply(lambda row: permute(row) if row['answer'] == 'Y' else None , axis=1)\n",
    "train['pairs']=train.apply(lambda row: [(row[col], row['unknown']) for col in KNOWN if row[col] is not None], axis=1)\n",
    "train['I_pairs']=train.apply(lambda row: [(row[col], row[col]) for col in FNAMES if row[col] is not None],axis=1)\n",
    "matched = match_unknowns(TRAINDATAPATH)\n",
    "matched['matched']=matched.apply(lambda row: [(row[col], row['unknown']) for col in KNOWN if row[col] is not None],axis=1)\n",
    "test['pairs']=test.apply(lambda row: [(row[col], row['unknown']) for col in KNOWN if row[col] is not None], axis=1)\n",
    "\n",
    "known = []\n",
    "unknown = []\n",
    "answers=[]\n",
    "pairs=train['pairs'].tolist()\n",
    "ans=train['answer']\n",
    "train['yes_pairs']=train['yes_pairs'].fillna('')\n",
    "yes_pairs = train['yes_pairs'].tolist()\n",
    "I_pairs = train['I_pairs'].tolist()\n",
    "\n",
    "for pair, yes_pair, ipair, a in zip(pairs, yes_pairs, I_pairs, ans):\n",
    "    for p in pair:\n",
    "        known.append(p[0])\n",
    "        unknown.append(p[1])\n",
    "        answers.append(a)\n",
    "    for p in ipair:\n",
    "        known.append(p[0])\n",
    "        unknown.append(p[1])\n",
    "        answers.append(a)\n",
    "    for p in yes_pair:\n",
    "        known.append(p[0])\n",
    "        unknown.append(p[1])\n",
    "        answers.append(a)\n",
    "\n",
    "\n",
    "ans = matched['answer']\n",
    "matched_pairs=matched['matched'].tolist()\n",
    "\n",
    "for matched_pair, a in zip(matched_pairs, ans):\n",
    "    for p in matched_pair:\n",
    "        known.append(p[0])\n",
    "        unknown.append(p[1])\n",
    "        answers.append(a)\n",
    "\n",
    "ans = test['answer']\n",
    "test_pairs=test['pairs'].tolist()\n",
    "ktest=[]\n",
    "utest=[]\n",
    "anstest=[]\n",
    "for test_pair, a in zip(test_pairs, ans):\n",
    "    for p in test_pair:\n",
    "        ktest.append(p[0])\n",
    "        utest.append(p[1])\n",
    "        anstest.append(a)\n",
    "        \n",
    "test_df = pd.DataFrame(data={\"label\":anstest, \"known\":ktest, \"unknown\":utest})\n",
    "test_df=test_df.sample(frac=1.0).reset_index(drop=True)\n",
    "train_df = pd.DataFrame(data={\"label\":answers, \"known\":known, \"unknown\":unknown})\n",
    "train_df=train_df.sample(frac=1.0).reset_index(drop=True)\n",
    "joblib.dump(train_df, 'data/train.pkl')\n",
    "joblib.dump(test_df, 'data/val.pkl')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! wget https://github.com/briandw/SiameseULMFiT/releases/download/1/data.zip\n",
    "#! unzip ./data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1140304\n"
     ]
    }
   ],
   "source": [
    "# load and process the all the sentences, just to get the LM trained\n",
    "raw_text = []\n",
    "for file in [f\"{snli_root}snli_1.0_train.jsonl\", f\"{snli_root}snli_1.0_dev.jsonl\", f\"{snli_root}snli_1.0_test.jsonl\"]:\n",
    "    with open(file) as fp:\n",
    "        while True:\n",
    "            line = fp.readline()\n",
    "            if line != None and len(line) > 0:\n",
    "                item = json.loads(line)\n",
    "                raw_text.append(item['sentence1'])\n",
    "                raw_text.append(item['sentence2'])\n",
    "            else:\n",
    "                break\n",
    "print(len(raw_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the language model data into train and validation sets\n",
    "lm_train, lm_valid = sklearn.model_selection.train_test_split(raw_text, test_size=0.1)\n",
    "df_trn = pd.DataFrame(lm_train)\n",
    "df_val = pd.DataFrame(lm_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = 'x_bos'  # beginning-of-sentence tag\n",
    "\n",
    "re1 = re.compile(r'  +')\n",
    "\n",
    "def fixup(x):\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))\n",
    "\n",
    "def get_texts(df):\n",
    "    texts = df[0].astype(str)\n",
    "    texts = list(texts.apply(fixup).values)\n",
    "    texts = f'{BOS} ' + df[0].astype(str)\n",
    "    tok = Tokenizer().proc_all_mp(partition_by_cores(texts))\n",
    "    return tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_trn = np.concatenate(get_texts(df_trn))\n",
    "tok_val = np.concatenate(get_texts(df_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['x_bos', 'people', 'are', 'in', 'a', 'room', 'discussing', 'around', 'a', 'computer', 'printer', '.',\n",
       "       'x_bos', 'the', 'girl', 'is', 'playing', 'on', 'a', 'swing', 'set', '.', 'x_bos', 'a', 'cleaning',\n",
       "       'woman', 'in', 'a', 'bright', 'uniform', 'is', 'pushing', 'a', 'cart', '.', 'x_bos', 'animals',\n",
       "       'playing', 'in', 'a', 'field', 'x_bos', 'a', 'person', 'is', 'taking', 'a', 'picture', 'of', 'some',\n",
       "       'kids', '.', 'x_bos', 'protesters', 'joining', 'on', 'a', 'city', 'street', '.', 'x_bos', 'bicyclist',\n",
       "       'riding', 'their', 'bikes', 'across', 'a', 'metal', 'bridge', '.', 'x_bos', 'a', 'man', 'is',\n",
       "       'holding', 'a', 'flashlight', '.', 'x_bos', 'the', 'team', 'swiftly', 'moves', 'their', 'traditional',\n",
       "       'boat', 'down', 'the', 'river', '.', 'x_bos', 'a', 'man', 'with', 'long', 'hair', 'and', 'a', 'pink',\n",
       "       'shirt'], dtype='<U17')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_val[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save our work\n",
    "np.save(f'{token_files}tok_trn.npy', tok_trn)\n",
    "np.save(f'{token_files}tok_val.npy', tok_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_trn = np.load(f'{token_files}tok_trn.npy')\n",
    "tok_val = np.load(f'{token_files}tok_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1496301),\n",
       " ('x_bos', 1140304),\n",
       " ('.', 999604),\n",
       " ('the', 555295),\n",
       " ('in', 423992),\n",
       " ('is', 387917),\n",
       " ('man', 276785),\n",
       " ('on', 245180),\n",
       " ('and', 215231),\n",
       " ('are', 206834),\n",
       " ('of', 200547),\n",
       " ('with', 176178),\n",
       " ('woman', 143101),\n",
       " ('two', 126950),\n",
       " ('people', 125650),\n",
       " (',', 119923),\n",
       " ('to', 118745),\n",
       " ('at', 102452),\n",
       " ('wearing', 84424),\n",
       " ('an', 83451),\n",
       " ('his', 75557),\n",
       " ('shirt', 65479),\n",
       " ('young', 64126),\n",
       " ('men', 63408),\n",
       " ('playing', 61568)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = Counter(np.concatenate([tok_trn, tok_val]))\n",
    "freq.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34158"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab = 60000\n",
    "min_freq = 1\n",
    "itos = [o for o, c in freq.most_common(max_vocab) if c>=min_freq]\n",
    "itos.insert(0, '_pad_')\n",
    "itos.insert(0, '_unk_')\n",
    "stoi = defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34160"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the language model training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_lm = np.array([stoi[p] for p in tok_trn])\n",
    "val_lm = np.array([stoi[p] for p in tok_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save results\n",
    "pickle.dump(itos, open(f'{token_files}itos.pkl', 'wb'))\n",
    "np.save(f'{token_files}trn_lm.npy', trn_lm)\n",
    "np.save(f'{token_files}val_lm.npy', val_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34160"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the results so we can pick it up from here \n",
    "itos = pickle.load(open(f'{token_files}itos.pkl', 'rb'))\n",
    "trn_lm = np.load(f'{token_files}trn_lm.npy')\n",
    "val_lm = np.load(f'{token_files}val_lm.npy')\n",
    "\n",
    "stoi = defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "vocab_size = len(itos)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_bos people are in a room discussing around a computer printer . x_bos the girl is playing on a swing set . x_bos a cleaning woman in a bright uniform is pushing a cart . x_bos animals playing in a field x_bos a person is taking a picture of some kids . x_bos protesters joining on a city street . x_bos bicyclist riding their bikes across a metal bridge . x_bos a man is holding a flashlight . x_bos the team swiftly moves their traditional boat down the river . x_bos a man with long hair and a pink shirt "
     ]
    }
   ],
   "source": [
    "for word in val_lm[:100]:\n",
    "    print(itos[word], end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the sentence similarity dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Entail(Enum):\n",
    "    entailment = 0\n",
    "    contradiction = 1\n",
    "    neutral = 2\n",
    "       \n",
    "def load_sentence_pairs(json_file):\n",
    "    content = []\n",
    "    with open(json_file) as fp:\n",
    "        while True:\n",
    "            line = fp.readline()\n",
    "            if line:\n",
    "                content.append(json.loads(line))\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    s0s = []\n",
    "    s1s = []\n",
    "    labels = []\n",
    "    avg_len = []\n",
    "    for item in content:\n",
    "        l = item['gold_label']\n",
    "        s0 = BOS+\" \"+fixup(item['sentence1'])\n",
    "        s1 = BOS+\" \"+fixup(item['sentence2'])\n",
    "\n",
    "        average_len = (len(s0)+len(s1))/2\n",
    "        try:\n",
    "            label = Entail[l].value\n",
    "            s0s.append(s0)\n",
    "            s1s.append(s1)\n",
    "            labels.append(label)\n",
    "            avg_len.append(average_len)\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "    s0s = Tokenizer().proc_all_mp(partition_by_cores(s0s))\n",
    "    s1s = Tokenizer().proc_all_mp(partition_by_cores(s1s))\n",
    "    return np.array((s0s, s1s, labels, avg_len)).transpose()    \n",
    "\n",
    "sentence_pairs_train = load_sentence_pairs(f'{snli_root}/snli_1.0_train.jsonl')\n",
    "sentence_pairs_dev = load_sentence_pairs(f'{snli_root}snli_1.0_dev.jsonl')\n",
    "sentence_pairs_test = load_sentence_pairs(f'{snli_root}snli_1.0_test.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'{token_files}trn_snli.npy', sentence_pairs_train)\n",
    "np.save(f'{token_files}dev_snli.npy', sentence_pairs_dev)\n",
    "np.save(f'{token_files}test_snli.npy', sentence_pairs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence_pairs):\n",
    "    for i in range(len(sentence_pairs)):\n",
    "        item = sentence_pairs[i]\n",
    "        tok0 = [stoi[p] for p in item[0]]\n",
    "        tok1 =[stoi[p] for p in item[1]]\n",
    "        sentence_pairs[i] = np.array([tok0, tok1, item[2], item[3]])\n",
    "\n",
    "tokenize(sentence_pairs_train)\n",
    "tokenize(sentence_pairs_dev)\n",
    "tokenize(sentence_pairs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'{token_files}snli_tok_train.npy', sentence_pairs_train)\n",
    "np.save(f'{token_files}snli_tok_dev.npy', sentence_pairs_dev)\n",
    "np.save(f'{token_files}snli_tok_test.npy', sentence_pairs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check our work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " x_bos a person on a horse jumps over a broken down airplane .\n",
      " x_bos a person is training his horse for a competition .\n",
      " x_bos two women are embracing while holding to go packages .\n",
      " x_bos the sisters are hugging goodbye while holding to go packages after just eating lunch .\n",
      " x_bos this church choir sings to the masses as they sing joyous songs from the book at a church .\n",
      " x_bos the church has cracks in the ceiling .\n"
     ]
    }
   ],
   "source": [
    "itos = pickle.load(open(f'{token_files}itos.pkl', 'rb'))\n",
    "\n",
    "dev = np.load(f'{token_files}snli_tok_dev.npy')\n",
    "train = np.load(f'{token_files}snli_tok_train.npy')\n",
    "test = np.load(f'{token_files}snli_tok_test.npy')\n",
    "\n",
    "def print_sentence(s):\n",
    "    sentence = \"\"\n",
    "    for tok in s:\n",
    "        sentence += \" \"+itos[tok]\n",
    "    print(sentence)\n",
    "\n",
    "print_sentence(train[0][0])\n",
    "print_sentence(train[0][1])\n",
    "\n",
    "print_sentence(dev[0][0])\n",
    "print_sentence(dev[0][1])\n",
    "\n",
    "print_sentence(test[0][0])\n",
    "print_sentence(test[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " x_bos two women are embracing while holding to go packages .\n",
      " x_bos the sisters are hugging goodbye while holding to go packages after just eating lunch .\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 15, 47, 11, 2243, 30, 48, 18, 381, 3644, 4]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
